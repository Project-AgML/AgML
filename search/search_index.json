{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#want-to-join-the-ai-institute-for-food-systems-team-and-help-lead-agml-development","title":"\ud83d\udc68\ud83c\udfff\u200d\ud83d\udcbb\ud83d\udc69\ud83c\udffd\u200d\ud83d\udcbb\ud83c\udf08\ud83e\udeb4 Want to join the AI Institute for Food Systems team and help lead AgML development? \ud83e\udeb4\ud83c\udf08\ud83d\udc69\ud83c\udffc\u200d\ud83d\udcbb\ud83d\udc68\ud83c\udffb\u200d\ud83d\udcbb","text":"<p>We're looking to hire a postdoc with both Python library development and ML experience. Send your resume and GitHub profile link to jmearles@ucdavis.edu!</p>"},{"location":"#overview","title":"Overview","text":"<p>AgML is a comprehensive library for agricultural machine learning. Currently, AgML provides access to a wealth of public agricultural datasets for common agricultural deep learning tasks. In the future, AgML will provide ag-specific ML functionality related to data, training, and evaluation. Here's a conceptual diagram of the overall framework. </p> <p> </p> <p>AgML supports both the TensorFlow and PyTorch machine learning frameworks.</p>"},{"location":"#installation","title":"Installation","text":"<p>To install the latest release of AgML, run the following command:</p> <pre><code>pip install agml\n</code></pre> <p>NOTE: Some features of AgML, such as synthetic data generation, require GUI applications. When running AgML through Windows Subsystem for Linux (WSL), it may be necessary to configure your WSL environment to utilize these features. Please follow the Microsoft documentation to install all necessary prerequisites and update WSL. The latest version of WSL includes built-in support for running Linux GUI applications.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>AgML is designed for easy usage of agricultural data in a variety of formats. You can start off by using the <code>AgMLDataLoader</code> to download and load a dataset into a container:</p> <pre><code>import agml\n\nloader = agml.data.AgMLDataLoader('apple_flower_segmentation')\n</code></pre> <p>You can then use the in-built processing methods to get the loader ready for your training and evaluation pipelines. This includes, but is not limited to, batching data, shuffling data, splitting data into training, validation, and test sets, and applying transforms. </p> <pre><code>import albumentations as A\n\n# Batch the dataset into collections of 8 pieces of data:\nloader.batch(8)\n\n# Shuffle the data:\nloader.shuffle()\n\n# Apply transforms to the input images and output annotation masks:\nloader.mask_to_channel_basis()\nloader.transform(\n    transform = A.RandomContrast(),\n    dual_transform = A.Compose([A.RandomRotate90()])\n)\n\n# Split the data into train/val/test sets.\nloader.split(train = 0.8, val = 0.1, test = 0.1)\n</code></pre> <p>The split datasets can be accessed using <code>loader.train_data</code>, <code>loader.val_data</code>, and <code>loader.test_data</code>. Any further processing applied to the main loader will be applied to the split datasets, until the split attributes are accessed, at which point you need to apply processing independently to each of the loaders. You can also turn toggle processing on and off using the <code>loader.eval()</code>, <code>loader.reset_preprocessing()</code>, and <code>loader.disable_preprocessing()</code> methods.</p> <p>You can visualize data using the <code>agml.viz</code> module, which supports multiple different types of visualization for different data types:</p> <pre><code># Disable processing and batching for the test data:\ntest_ds = loader.test_data\ntest_ds.batch(None)\ntest_ds.reset_prepreprocessing()\n\n# Visualize the image and mask side-by-side:\nagml.viz.visualize_image_and_mask(test_ds[0])\n\n# Visualize the mask overlaid onto the image:\nagml.viz.visualize_overlaid_masks(test_ds[0])\n</code></pre> <p>AgML supports both the TensorFlow and PyTorch libraries as backends, and provides functionality to export your loaders to native TensorFlow and PyTorch formats when you want to use them in a training pipeline. This includes both exporting the <code>AgMLDataLoader</code> to a <code>tf.data.Dataset</code> or <code>torch.utils.data.DataLoader</code>, but also internally converting data within the <code>AgMLDataLoader</code> itself, enabling access to its core functionality.</p> <pre><code># Export the loader as a `tf.data.Dataset`:\ntrain_ds = loader.train_data.export_tensorflow()\n\n# Convert to PyTorch tensors without exporting.\ntrain_ds = loader.train_data\ntrain_ds.as_torch_dataset()\n</code></pre> <p>You're now ready to use AgML for training your own models! Luckily, AgML comes with a training module that enables quick-start training of standard deep learning models on agricultural datasets. Training a grape detection model is as simple as the following code:</p> <pre><code>import agml\nimport agml.models\n\nimport albumentations as A\n\nloader = agml.data.AgMLDataLoader('grape_detection_californiaday')\nloader.split(train = 0.8, val = 0.1, test = 0.1)\nprocessor = agml.models.preprocessing.EfficientDetPreprocessor(\n    image_size = 512, augmentation = [A.HorizontalFlip(p=0.5)]\n)\nloader.transform(processor)\n\nmodel = agml.models.DetectionModel(num_classes=loader.num_classes)\n\nmodel.run_training(loader)\n</code></pre>"},{"location":"#public-dataset-listing","title":"Public Dataset Listing","text":"Dataset Task Number of Images bean_disease_uganda Image Classification 1295 carrot_weeds_germany Semantic Segmentation 60 plant_seedlings_aarhus Image Classification 5539 soybean_weed_uav_brazil Image Classification 15336 sugarcane_damage_usa Image Classification 153 crop_weeds_greece Image Classification 508 sugarbeet_weed_segmentation Semantic Segmentation 1931 rangeland_weeds_australia Image Classification 17509 fruit_detection_worldwide Object Detection 565 leaf_counting_denmark Image Classification 9372 apple_detection_usa Object Detection 2290 mango_detection_australia Object Detection 1730 apple_flower_segmentation Semantic Segmentation 148 apple_segmentation_minnesota Semantic Segmentation 670 rice_seedling_segmentation Semantic Segmentation 224 plant_village_classification Image Classification 55448 autonomous_greenhouse_regression Image Regression 389 grape_detection_syntheticday Object Detection 448 grape_detection_californiaday Object Detection 126 grape_detection_californianight Object Detection 150 guava_disease_pakistan Image Classification 306 apple_detection_spain Object Detection 967 apple_detection_drone_brazil Object Detection 689 plant_doc_classification Image Classification 2598 plant_doc_detection Object Detection 2346 wheat_head_counting Object Detection 6512 peachpear_flower_segmentation Semantic Segmentation 42 red_grapes_and_leaves_segmentation Semantic Segmentation 258 white_grapes_and_leaves_segmentation Semantic Segmentation 273 ghai_romaine_detection Object Detection 500 ghai_green_cabbage_detection Object Detection 500 ghai_iceberg_lettuce_detection Object Detection 500 riseholme_strawberry_classification_2021 Image Classification 3520 ghai_broccoli_detection Object Detection 500 bean_synthetic_earlygrowth_aerial Semantic Segmentation 2500 ghai_strawberry_fruit_detection Object Detection 500 vegann_multicrop_presence_segmentation Semantic Segmentation 3775 corn_maize_leaf_disease Image Classification 4188 tomato_leaf_disease Image Classification 11000 vine_virus_photo_dataset Image Classification 3866"},{"location":"#usage-information","title":"Usage Information","text":""},{"location":"#using-public-agricultural-data","title":"Using Public Agricultural Data","text":"<p>AgML aims to provide easy access to a range of existing public agricultural datasets The core of AgML's public data pipeline is  <code>AgMLDataLoader</code>. You can use the <code>AgMLDataLoader</code> or <code>agml.data.download_public_dataset()</code> to download  the dataset locally from which point it will be automatically loaded from the disk on future runs.  From this point, the data within the loader can be split into train/val/test sets, batched, have augmentations and transforms applied, and be converted into a training-ready dataset (including batching, tensor conversion, and image formatting).</p> <p>To see the various ways in which you can use AgML datasets in your training pipelines, check out  the example notebook.</p>"},{"location":"#annotation-formats","title":"Annotation Formats","text":"<p>A core aim of AgML is to provide datasets in a standardized format, enabling the synthesizing of multiple datasets into a single training pipeline. To this end, we provide annotations in the following formats:</p> <ul> <li>Image Classification: Image-To-Label-Number</li> <li>Object Detection: COCO JSON</li> <li>Semantic Segmentation: Dense Pixel-Wise</li> </ul>"},{"location":"#contributions","title":"Contributions","text":"<p>We welcome contributions! If you would like to contribute a new feature, fix an issue that you've noticed, or even just mention a bug or feature that you would like to see implemented, please don't hesitate to use the Issues tab to bring it to our attention. See the contributing guidelines for more information.</p>"},{"location":"#funding","title":"Funding","text":"<p>This project is partly funded by the National AI Institute for Food Systems</p>"},{"location":"credits/","title":"Credits","text":""},{"location":"credits/#exec-1--credits","title":"Credits","text":"<p>These projects were used to build agml. Thank you!</p> <p>Python | uv |</p>"},{"location":"credits/#exec-1--runtime-dependencies","title":"Runtime dependencies","text":"Project Summary Version (accepted) Version (last resolved) License albucore A high-performance image processing library designed to optimize and extend the Albumentations library with specialized functions for advanced image transformations. Perfect for developers working in computer vision who require efficient and scalable image augmentation. <code>==0.0.17</code> <code>0.0.17</code> MIT albumentations Fast, flexible, and advanced image augmentation library for deep learning and computer vision. Albumentations offers a wide range of transformations for images, masks, bounding boxes, and keypoints, with optimized performance and seamless integration into ML workflows. <code>&gt;=1.4.18</code> <code>1.4.18</code> MIT License annotated-types Reusable constraint types to use with typing.Annotated <code>&gt;=0.6.0</code> <code>0.7.0</code> MIT License certifi Python package for providing Mozilla's CA Bundle. <code>&gt;=2017.4.17</code> <code>2024.8.30</code> MPL-2.0 charset-normalizer The Real First Universal Charset Detector. Open, modern and actively maintained alternative to Chardet. <code>&gt;=2, &lt;4</code> <code>3.4.0</code> MIT colorama Cross-platform colored terminal text. <code>0.4.6</code> BSD License contourpy Python library for calculating contours of 2D quadrilateral grids <code>&gt;=1.0.1</code> <code>1.1.1</code> BSD License cycler Composable style cycles <code>&gt;=0.10</code> <code>0.12.1</code> BSD License dict2xml Small utility to convert a python dictionary into an XML string <code>&gt;=1.7.6</code> <code>1.7.6</code> MIT eval_type_backport Like <code>typing._eval_type</code>, but lets older Python versions use newer typing features. <code>0.2.0</code> MIT fonttools Tools to manipulate font files <code>&gt;=4.22.0</code> <code>4.55.0</code> MIT idna Internationalized Domain Names in Applications (IDNA) <code>&gt;=2.5, &lt;4</code> <code>3.10</code> BSD License imageio Library for reading and writing a wide range of image, video, scientific, and volumetric data formats. <code>&gt;=2.27</code> <code>2.35.1</code> BSD-2-Clause joblib Lightweight pipelining with Python functions <code>&gt;=1.1.1</code> <code>1.4.2</code> BSD 3-Clause kiwisolver A fast implementation of the Cassowary constraint solver <code>&gt;=1.0.1</code> <code>1.4.7</code> BSD License lazy_loader Makes it easy to load subpackages and functions on demand. <code>&gt;=0.2</code> <code>0.4</code> BSD License matplotlib Python plotting package <code>&gt;=3.7.5</code> <code>3.7.5</code> PSF networkx Python package for creating and manipulating graphs and networks <code>&gt;=2.8</code> <code>3.1</code> BSD License numpy Fundamental package for array computing in Python <code>&gt;=1.24.4, &gt;=1.21.1</code> <code>1.24.4</code> BSD-3-Clause opencv-python Wrapper package for OpenCV python bindings. <code>&gt;=4.10.0.84</code> <code>4.10.0.84</code> Apache 2.0 opencv-python-headless Wrapper package for OpenCV python bindings. <code>&gt;=4.10.0.84</code> <code>4.10.0.84</code> Apache 2.0 packaging Core utilities for Python packages <code>&gt;=20.0</code> <code>24.2</code> Apache Software License + BSD License pillow Python Imaging Library (Fork) <code>&gt;=9.0.1, &gt;=6.2.0</code> <code>10.4.0</code> HPND pydantic Data validation using Python type hints <code>&gt;=2.7.0</code> <code>2.10.0</code> MIT pydantic_core Core functionality for Pydantic validation and serialization <code>==2.27.0</code> <code>2.27.0</code> MIT pyparsing pyparsing module - Classes and methods to define and execute parsing grammars <code>&gt;=2.3.1</code> <code>3.1.4</code> MIT License python-dateutil Extensions to the standard Python datetime module <code>&gt;=2.8.2, &gt;=2.7</code> <code>2.9.0.post0</code> BSD License + Apache Software License PyWavelets PyWavelets, wavelet transform module <code>&gt;=1.1.1</code> <code>1.4.1</code> MIT PyYAML YAML parser and emitter for Python <code>&gt;=6.0.2</code> <code>6.0.2</code> MIT requests Python HTTP for Humans. <code>&gt;=2.0.0</code> <code>2.32.3</code> Apache-2.0 scikit-image Image processing in Python <code>&gt;=0.21.0</code> <code>0.21.0</code> BSD License scikit-learn A set of python modules for machine learning and data mining <code>&gt;=1.3.2</code> <code>1.3.2</code> new BSD scipy Fundamental algorithms for scientific computing in Python <code>&gt;=1.8, &gt;=1.5.0</code> <code>1.10.1</code> BSD License six Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.16.0</code> MIT threadpoolctl threadpoolctl <code>&gt;=2.0.0</code> <code>3.5.0</code> BSD-3-Clause tifffile Read and write TIFF files <code>&gt;=2022.8.12</code> <code>2023.7.10</code> BSD tqdm Fast, Extensible Progress Meter <code>&gt;=4.67.0</code> <code>4.67.0</code> MPL-2.0 AND MIT typing_extensions Backported and Experimental Type Hints for Python 3.8+ <code>&gt;=4.9.0, &gt;=4.6.0</code> <code>4.12.2</code> Python Software Foundation License urllib3 HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.25.4, &gt;=1.21.1, &lt;3, &lt;1.27</code> <code>1.26.20</code> MIT"},{"location":"credits/#exec-1--development-dependencies","title":"Development dependencies","text":"Project Summary Version (accepted) Version (last resolved) License attrs Classes Without Boilerplate <code>24.2.0</code> MIT boto3 The AWS SDK for Python <code>&gt;=1.35.66</code> <code>1.35.66</code> Apache License 2.0 botocore Low-level, data-driven core of boto 3. <code>&gt;=1.35.66</code> <code>1.35.66</code> Apache License 2.0 click Composable command line interface toolkit <code>&gt;=7.1</code> <code>8.1.7</code> BSD-3-Clause colorama Cross-platform colored terminal text. <code>0.4.6</code> BSD License coverage Code coverage measurement for Python <code>&gt;=7.6.1</code> <code>7.6.1</code> Apache-2.0 exceptiongroup Backport of PEP 654 (exception groups) <code>&gt;=1.0.0rc8</code> <code>1.2.2</code> MIT License imageio Library for reading and writing a wide range of image, video, scientific, and volumetric data formats. <code>&gt;=2.27</code> <code>2.35.1</code> BSD-2-Clause iniconfig brain-dead simple config-ini parsing <code>2.0.0</code> MIT interrogate Interrogate a codebase for docstring coverage. <code>&gt;=1.7.0</code> <code>1.7.0</code> MIT License jmespath JSON Matching Expressions <code>&gt;=0.7.1, &lt;2.0.0</code> <code>1.0.1</code> MIT lazy_loader Makes it easy to load subpackages and functions on demand. <code>&gt;=0.2</code> <code>0.4</code> BSD License mypy Optional static typing for Python <code>&gt;=1.13.0</code> <code>1.13.0</code> MIT mypy-extensions Type system extensions for programs checked with the mypy type checker. <code>&gt;=1.0.0</code> <code>1.0.0</code> MIT License networkx Python package for creating and manipulating graphs and networks <code>&gt;=2.8</code> <code>3.1</code> BSD License numpy Fundamental package for array computing in Python <code>&gt;=1.24.4, &gt;=1.21.1</code> <code>1.24.4</code> BSD-3-Clause packaging Core utilities for Python packages <code>&gt;=20.0</code> <code>24.2</code> Apache Software License + BSD License pandas Powerful data structures for data analysis, time series, and statistics <code>&gt;=2.0.3</code> <code>2.0.3</code> BSD License pillow Python Imaging Library (Fork) <code>&gt;=9.0.1, &gt;=6.2.0</code> <code>10.4.0</code> HPND pluggy plugin and hook calling mechanisms for python <code>&gt;=1.5, &lt;2</code> <code>1.5.0</code> MIT py library with cross-python path, ini-parsing, io, code, log facilities <code>1.11.0</code> MIT license pytest pytest: simple powerful testing with Python <code>&gt;=8.3.3</code> <code>8.3.3</code> MIT pytest-cov Pytest plugin for measuring coverage. <code>&gt;=5.0.0</code> <code>5.0.0</code> MIT pytest-order pytest plugin to run your tests in a specific order <code>&gt;=1.3.0</code> <code>1.3.0</code> MIT python-dateutil Extensions to the standard Python datetime module <code>&gt;=2.8.2, &gt;=2.7</code> <code>2.9.0.post0</code> BSD License + Apache Software License pytz World timezone definitions, modern and historical <code>&gt;=2020.1</code> <code>2024.2</code> MIT PyWavelets PyWavelets, wavelet transform module <code>&gt;=1.1.1</code> <code>1.4.1</code> MIT ruff An extremely fast Python linter and code formatter, written in Rust. <code>&gt;=0.7.4</code> <code>0.7.4</code> MIT s3transfer An Amazon S3 Transfer Manager <code>&gt;=0.10.0, &lt;0.11.0</code> <code>0.10.4</code> Apache License 2.0 scikit-image Image processing in Python <code>&gt;=0.21.0</code> <code>0.21.0</code> BSD License scipy Fundamental algorithms for scientific computing in Python <code>&gt;=1.8, &gt;=1.5.0</code> <code>1.10.1</code> BSD License shapely Manipulation and analysis of geometric objects <code>&gt;=2.0.6</code> <code>2.0.6</code> BSD 3-Clause six Python 2 and 3 compatibility utilities <code>&gt;=1.5</code> <code>1.16.0</code> MIT tabulate Pretty-print tabular data <code>0.9.0</code> MIT tifffile Read and write TIFF files <code>&gt;=2022.8.12</code> <code>2023.7.10</code> BSD tomli A lil' TOML parser <code>&gt;=1</code> <code>2.1.0</code> MIT License typing_extensions Backported and Experimental Type Hints for Python 3.8+ <code>&gt;=4.9.0, &gt;=4.6.0</code> <code>4.12.2</code> Python Software Foundation License tzdata Provider of IANA time zone data <code>&gt;=2022.1</code> <code>2024.2</code> Apache-2.0 urllib3 HTTP library with thread-safe connection pooling, file post, and more. <code>&gt;=1.25.4, &gt;=1.21.1, &lt;3, &lt;1.27</code> <code>1.26.20</code> MIT"},{"location":"developement/","title":"Contributing Guidelines","text":"<p>Thank you for choosing to contribute to AgML! </p>"},{"location":"developement/#contributing-data","title":"Contributing Data","text":"<p>If you've found (or already have) a new dataset and you want to contribute the dataset to AgML, then the instructions below will help you format and the data to the AgML standard.</p>"},{"location":"developement/#dataset-formats","title":"Dataset Formats","text":"<p>Currently, we have image classification, object detection, and semantic segmentation datasets available in AgML. These sources are synthesized to standard annotation formats, namely the following:</p> <ul> <li>Image Classification: Image-To-Label-Number</li> <li>Object Detection: COCO JSON</li> <li>Semantic Segmentation: Dense Pixel-Wise</li> </ul>"},{"location":"developement/#image-classification","title":"Image Classification","text":"<p>Image classification datasets are organized in the following directory tree:</p> <pre><code>&lt;dataset name&gt;\n    \u251c\u2500\u2500 &lt;label 1&gt;\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 image1.png\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 image2.png\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 image3.png\n    \u2514\u2500\u2500 &lt;label 2&gt;\n        \u251c\u2500\u2500 image1.png\n        \u251c\u2500\u2500 image2.png\n        \u2514\u2500\u2500 image3.png               \n</code></pre> <p>The <code>AgMLDataLoader</code> generates a mapping between each of the label names \"label 1\", \"label 2\", etc., and a numerical value. </p>"},{"location":"developement/#object-detection","title":"Object Detection","text":"<p>Object detection datasets are constructed using COCO JSON formatting. For a general overview, see  https://cocodataset.org/#format-data. Another good resource is https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/cd-transform-coco.html. Once you have the images and the bounding box annotations, this involves generating a dictionary with four keys:</p> <ol> <li><code>images</code>: A list of dictionaries with the following items:<ul> <li>The image file name (without the parent directory!) in <code>file_name</code></li> <li>The ID (a unique number, usually from 1 to num_images) in <code>id</code>,</li> <li>The height/width of the image in <code>height</code> and <code>width</code>, respectively.</li> </ul> </li> <li><code>annotations</code>: A list of dictionaries with each dictionary representing a unique bounding box (do not stack multiple bounding boxes into a single dictionary, even if they are for the same image!), and containing:<ul> <li>The area of the bounding box in <code>area</code>.</li> <li>The bounding box itself in <code>bbox</code>. Note: The bounding box should have four coordinates. The first two are the x, y of the top-left corner of the bounding box, the other two are its height and width.</li> <li>The class label (numerical) of the image in <code>category_id</code>.</li> <li>The ID (NOT the filename) of the image it corresponds to in <code>image_id</code>.</li> <li>The ID of the bounding box in <code>id</code>. For instance, if a unique image has six corresponding bounding boxes, then each of them would be given an <code>id</code> from 1-6.</li> <li><code>iscrowd</code> should be set to 0 by default, unless the dataset explicitly comes with <code>iscrowd</code> as 1.</li> <li><code>ignore</code> should be 0 by default.</li> <li><code>segmentation</code> only applies for instance segmentation datasets. If converting an instance segmentation dataset to object detection, you can leave the polygonal segmentation as is. Otherwise, put this as an empty list.</li> </ul> </li> <li><code>category</code>: A list of dictionaries with each category, where each of these dictionaries contains:<ul> <li>The human-readable name of the class (e.g., \"strawberry\") in <code>name</code>.</li> <li>The supercategory of the class, if there are nested classes, in <code>supercategory</code>. Otherwise, just leave this as the string <code>\"none\"</code>.</li> <li>The numerical ID of the class in <code>id</code>.</li> </ul> </li> <li><code>info</code>: A single dictionary with metadata and information about the dataset:<ul> <li><code>description</code>: A basic description of the dataset.</li> <li><code>url</code>: The URL from which the dataset was acquired.</li> <li><code>version</code>: The dataset version. Set to <code>1.0</code> if unknown.</li> <li><code>year</code>: The year in which the dataset was released.</li> <li><code>contributor</code>: The author(s) of the dataset.</li> <li><code>date_created</code>: The date when the dataset was published. Give an approximate year if unknown.</li> </ul> </li> </ol> <p>The dictionary containing this information should be written to a file called <code>annotations.json</code>, and the file structure will be:</p> <pre><code>&lt;dataset name&gt;\n    \u251c\u2500\u2500 annotations.json\n    \u2514\u2500\u2500 images\n        \u251c\u2500\u2500 image1.png\n        \u251c\u2500\u2500 image2.png\n        \u2514\u2500\u2500 image3.png              \n</code></pre>"},{"location":"developement/#semantic-segmentation","title":"Semantic Segmentation","text":"<p>Semantic segmentation datasets are constructed using pixel-wise annotation masks. Each image in the dataset has a corresponding annotation mask. These masks have the following properties:</p> <ol> <li>Two-dimensional, so no channel shape. Their complete shape will be <code>(image_height, image_width)</code>.</li> <li>Each of the pixels will be a numerical class label or <code>0</code> for background.</li> </ol> <p>The directory tree should look like follows:</p> <pre><code>&lt;dataset name&gt;\n    \u251c\u2500\u2500 annotations\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 mask1.png\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 mask2.png\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 mask3.png\n    \u2514\u2500\u2500 images\n        \u251c\u2500\u2500 image1.png\n        \u251c\u2500\u2500 image2.png\n        \u2514\u2500\u2500 image3.png\n</code></pre>"},{"location":"developement/#contributing-a-dataset","title":"Contributing a Dataset","text":"<p>If you've found a new dataset that isn't already being used in AgML and you want to add it, there's a few things you need to do. </p> <p>Any preprocessing code being used for the dataset can be kept in <code>agml/_internal/preprocess.py</code>, by adding an <code>elif</code> statement to the <code>preprocess()</code> method with the dataset name. If there is no preprocessing code, then just put a <code>pass</code> statement in the block.</p>"},{"location":"developement/#some-things-to-check","title":"Some Things to Check","text":"<ul> <li>Make sure each image is in the range of 0-255 in integers as opposed to 0-1 as floats. This will prevent any loss of data that   could adversely affect training.</li> <li>For a semantic segmentation dataset, put the masks in a <code>png</code> format as opposed to <code>jpg</code> or other.</li> </ul>"},{"location":"developement/#compiling-the-dataset","title":"Compiling the Dataset","text":"<p>After processing and standardizing the dataset, make sure that the dataset is organized in one of the formats above, and then go to the parent directory of the directory of the dataset (for example, if the dataset is in <code>/root/my_new_dataset</code>, go to <code>/root</code>). Then run the following command:</p> <pre><code>zip -r my_new_dataset.zip my_new_dataset -x \".*\"\n</code></pre> <p>If running on MacOS, use the following command:</p> <pre><code>zip -r my_new_dataset.zip my_new_dataset -x \".*\"  -x \"__MACOSX\"\n</code></pre>"},{"location":"developement/#updating-the-source-files","title":"Updating the Source Files","text":"<p>Next, you need to update the <code>public_datasources.json</code> and <code>source_citations.json</code> files. These two can be found in the <code>agml/_assets</code> folder. You will need to update the <code>public_datasources.json</code> file in the following way:</p> <pre><code>    \"my_new_dataset\": {\n        \"classes\": {\n            \"1\": \"class_1\",\n            \"2\": \"class_2\",\n            \"3\": \"class_3\"\n        },\n        \"ml_task\": \"See the table for the different dataset types.\",\n        \"ag_task\": \"The agricultural task that is associated with the dataset.\",\n        \"location\": {\n            \"continent\": \"The continent the dataset was collected on.\",\n            \"country\": \"The country the dataset was collected in.\"\n        },\n        \"sensor_modality\": \"Usually rgb, but can include other image modalities.\",\n        \"real_synthetic\": \"Are the images real or synthetically generated?\",\n        \"platform\": \"handheld or ground\",\n        \"input_data_format\": \"See the table for the different dataset types.\",\n        \"annotation_format\": \"See the table for the different dataset types.\",\n        \"n_images\": \"The total number of images in the dataset.\",\n        \"docs_url\": \"Where can the user find the most clear information about the dataset?\"\n    }\n</code></pre> <p>Note: If the dataset is captured in multiple countries or you don't know where it is from, then put \"worldwide\" for both \"continent\" and \"country\".</p> Dataset Format <code>ml_task</code> <code>annotation_format</code> Image Classification <code>image_classification</code> <code>directory_names</code> Object Detection <code>object_detection</code> <code>coco_json</code> Semantic Segmentation <code>semantic_segmentation</code> <code>image</code> <p>The <code>source_citations.json</code> file should be updated this way:</p> <pre><code>    \"my_new_dataset\": {\n        \"license\": \"The license being used by the dataset.\",\n        \"citation\": \"The paper/library to cite for the dataset.\"\n    }\n</code></pre> <p>If the dataset has no license or has no citation, leave the corresponding lines blank.</p>"},{"location":"developement/#uploading-the-dataset","title":"Uploading the Dataset","text":"<p>Once you've readied the dataset, create a new pull request on the AgML repository. We will then review the changes and review next steps for adding the dataset into AgML's public data storage.</p>"},{"location":"developement/#developement-guidelines","title":"Developement Guidelines","text":""},{"location":"developement/#installing-uv","title":"Installing uv","text":"<p>Install uv follow the guidelines in  https://docs.astral.sh/uv/getting-started/installation/, it is recommended to use the standalone installation.</p>"},{"location":"developement/#building-project","title":"Building Project","text":"<p>The build the associated wheels simply run:</p> <pre><code>uv build\n</code></pre> <p>To sync the dependencies simply run:</p> <pre><code>uv sync\n</code></pre>"},{"location":"developement/#running-scripts","title":"Running scripts","text":"<p>For running scripts or one using the project's e</p> <pre><code>uv run python &lt;script&gt;\n</code></pre>"},{"location":"license/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright yyyy</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"datasets/apple_detection_drone_brazil/","title":"<code>apple_detection_drone_brazil</code>","text":""},{"location":"datasets/apple_detection_drone_brazil/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes apple Machine Learning Task object_detection Agricultural Task fruit_detection Location Brazil, South America Sensor Modality RGB Platform ground Input Data Format JPG Annotation Format coco_json Number of Images 689 Documentation https://github.com/thsant/add256/tree/zenodo-1.0 Stats/Mean [0.336, 0.482, 0.32] Stats/Standard Deviation [0.184, 0.187, 0.173]"},{"location":"datasets/apple_detection_drone_brazil/#examples","title":"Examples","text":""},{"location":"datasets/apple_detection_spain/","title":"<code>apple_detection_spain</code>","text":""},{"location":"datasets/apple_detection_spain/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes apple Machine Learning Task object_detection Agricultural Task fruit_detection Location Spain, Europe Sensor Modality RGB Platform ground Input Data Format JPG Annotation Format coco_json Number of Images 967 Documentation https://www.grap.udl.cat/en/publications/KFuji_RGBDS_database.html Stats/Mean [0.375, 0.47, 0.393] Stats/Standard Deviation [0.272, 0.28, 0.283]"},{"location":"datasets/apple_detection_spain/#examples","title":"Examples","text":""},{"location":"datasets/apple_detection_usa/","title":"<code>apple_detection_usa</code>","text":""},{"location":"datasets/apple_detection_usa/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task object_detection Agricultural Task fruit_detection Location United States, North America Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format PNG Annotation Format coco_json Number of Images 2290 Documentation https://hdl.handle.net/2376/17721 Stats/Mean [0.281, 0.29, 0.278] Stats/Standard Deviation [0.189, 0.186, 0.189] Classes apple"},{"location":"datasets/apple_detection_usa/#examples","title":"Examples","text":""},{"location":"datasets/apple_flower_segmentation/","title":"<code>apple_flower_segmentation</code>","text":""},{"location":"datasets/apple_flower_segmentation/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task semantic_segmentation Agricultural Task flower_segmentation Location United States, North America Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format JPG Annotation Format image Number of Images 148 Documentation https://data.nal.usda.gov/dataset/data-multi-species-fruit-flower-detection-using-refined-semantic-segmentation-network Stats/Mean [0.458, 0.479, 0.405] Stats/Standard Deviation [0.204, 0.193, 0.216] Classes apple"},{"location":"datasets/apple_flower_segmentation/#examples","title":"Examples","text":""},{"location":"datasets/apple_segmentation_minnesota/","title":"<code>apple_segmentation_minnesota</code>","text":""},{"location":"datasets/apple_segmentation_minnesota/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task semantic_segmentation Agricultural Task fruit_segmentation Location United States, North America Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format PNG Annotation Format image Number of Images 670 Documentation https://rsn.umn.edu/projects/orchard-monitoring/minneapple Stats/Mean [0.374, 0.437, 0.338] Stats/Standard Deviation [0.263, 0.288, 0.331] Classes apple"},{"location":"datasets/apple_segmentation_minnesota/#examples","title":"Examples","text":""},{"location":"datasets/bean_disease_uganda/","title":"<code>bean_disease_uganda</code>","text":""},{"location":"datasets/bean_disease_uganda/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task image_classification Agricultural Task disease_classification Location Uganda, Africa Sensor Modality RGB Real or Synthetic real Platform handheld Input Data Format JPG Annotation Format directory_names Number of Images 1295 Documentation https://github.com/AI-Lab-Makerere/ibean/ Classes angular_leaf_spot, bean_rust, healthy Stats/Mean [0.485, 0.519, 0.311] Stats/Standard Deviation [0.182, 0.199, 0.169]"},{"location":"datasets/bean_disease_uganda/#examples","title":"Examples","text":""},{"location":"datasets/bean_synthetic_earlygrowth_aerial/","title":"<code>bean_synthetic_earlygrowth_aerial</code>","text":""},{"location":"datasets/bean_synthetic_earlygrowth_aerial/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes leaves, branches Machine Learning Task semantic_segmentation Agricultural Task plant_segmentation Location Digital, Digital Sensor Modality RGB Real or Synthetic synthetic Platform aerial Input Data Format JPG Annotation Format image Number of Images 2500 Documentation https://github.com/Project-AgML/AgML/blob/main/docs/datasets/bean_synthetic_earlygrowth_aerial.md"},{"location":"datasets/bean_synthetic_earlygrowth_aerial/#examples","title":"Examples","text":""},{"location":"datasets/carrot_weeds_germany/","title":"<code>carrot_weeds_germany</code>","text":""},{"location":"datasets/carrot_weeds_germany/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task semantic_segmentation Agricultural Task weed_segmentation Location Germany, Europe Sensor Modality RGB Real or Synthetic real Platform ground_mobile Input Data Format PNG Annotation Format image Number of Images 60 Documentation https://github.com/cwfid/dataset Stats/Mean [0.274, 0.311, 0.274] Stats/Standard Deviation [0.086, 0.1, 0.086] Classes carrot, weeds"},{"location":"datasets/carrot_weeds_germany/#examples","title":"Examples","text":""},{"location":"datasets/corn_maize_leaf_disease/","title":"<code>corn_maize_leaf_disease</code>","text":""},{"location":"datasets/corn_maize_leaf_disease/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Blight, Common_Rust, Gray_Leaf_Spot, Healthy Machine Learning Task image_classification Agricultural Task disease_classification Location Worldwide Sensor Modality RGB Real or Synthetic real Platform handheld Input Data Format jpg, png, jpeg, JPG Annotation Format directory_names Number of Images 4188 Documentation https://www.kaggle.com/datasets/smaranjitghose/corn-or-maize-leaf-disease-dataset/data Stats/Mean [0.438, 0.498, 0.376] Stats/Standard Deviation [0.179, 0.167, 0.174]"},{"location":"datasets/corn_maize_leaf_disease/#examples","title":"Examples","text":""},{"location":"datasets/crop_weeds_greece/","title":"<code>crop_weeds_greece</code>","text":""},{"location":"datasets/crop_weeds_greece/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task image_classification Agricultural Task weed_classification Location Greece, Europe Sensor Modality RGB Platform handheld Input Data Format JPG Annotation Format directory_names Number of Images 508 Documentation https://github.com/AUAgroup/early-crop-weed Classes black_nightshade, velvet_leaf, cotton, tomato Stats/Mean [0.725, 0.669, 0.544] Stats/Standard Deviation [0.164, 0.153, 0.138]"},{"location":"datasets/crop_weeds_greece/#examples","title":"Examples","text":""},{"location":"datasets/fruit_detection_worldwide/","title":"<code>fruit_detection_worldwide</code>","text":""},{"location":"datasets/fruit_detection_worldwide/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task object_detection Agricultural Task fruit_detection Location Worldwide Sensor Modality RGB Real or Synthetic real Platform mixed Input Data Format JPG Annotation Format coco_json Number of Images 565 Documentation https://drive.google.com/drive/folders/1CmsZb1caggLRN7ANfika8WuPiywo4mBb Stats/Mean [0.378, 0.409, 0.287] Stats/Standard Deviation [0.231, 0.225, 0.218] Classes avocado, rockmelon, apple, orange, strawberry, mango, capsicum"},{"location":"datasets/fruit_detection_worldwide/#examples","title":"Examples","text":""},{"location":"datasets/ghai_broccoli_detection/","title":"<code>ghai_broccoli_detection</code>","text":""},{"location":"datasets/ghai_broccoli_detection/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes canopy, crown Machine Learning Task object_detection Agricultural Task crop_detection Location United States, North America Sensor Modality RGB Real or Synthetic real Platform handheld/ground Input Data Format JPG Annotation Format coco_json Number of Images 500 Documentation https://github.com/AxisAg/GHAIDatasets/blob/main/datasets/broccoli.md"},{"location":"datasets/ghai_broccoli_detection/#examples","title":"Examples","text":""},{"location":"datasets/ghai_strawberry_fruit_detection/","title":"<code>ghai_strawberry_fruit_detection</code>","text":""},{"location":"datasets/ghai_strawberry_fruit_detection/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Bud, Calyx, Detached Fruit, Flower, Large green, Leaf, Ripe fruit, Small Green, Stem, Unripe fruit Machine Learning Task object_detection Agricultural Task crop_detection Location United States, North America Sensor Modality RGB Real or Synthetic real Platform handheld/ground Input Data Format JPG Annotation Format coco_json Number of Images 500 Documentation https://github.com/AxisAg/GHAIDatasets/blob/main/datasets/strawberry.md"},{"location":"datasets/ghai_strawberry_fruit_detection/#examples","title":"Examples","text":""},{"location":"datasets/grape_detection_californiaday/","title":"<code>grape_detection_californiaday</code>","text":""},{"location":"datasets/grape_detection_californiaday/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes grape Machine Learning Task object_detection Agricultural Task fruit_detection Location United States, North America Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format JPG Annotation Format image Number of Images 126 Documentation None Stats/Mean [0.446, 0.464, 0.275] Stats/Standard Deviation [0.268, 0.259, 0.263]"},{"location":"datasets/grape_detection_californiaday/#examples","title":"Examples","text":""},{"location":"datasets/grape_detection_californianight/","title":"<code>grape_detection_californianight</code>","text":""},{"location":"datasets/grape_detection_californianight/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes grape Machine Learning Task object_detection Agricultural Task fruit_detection Location United States, North America Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format JPG Annotation Format image Number of Images 150 Documentation None Stats/Mean [0.222, 0.213, 0.225] Stats/Standard Deviation [0.205, 0.201, 0.206]"},{"location":"datasets/grape_detection_californianight/#examples","title":"Examples","text":""},{"location":"datasets/grape_detection_syntheticday/","title":"<code>grape_detection_syntheticday</code>","text":""},{"location":"datasets/grape_detection_syntheticday/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes grape Machine Learning Task object_detection Agricultural Task fruit_detection Location United States, North America Sensor Modality RGB Real or Synthetic synthetic Platform ground Input Data Format JPG Annotation Format coco_json Number of Images 448 Documentation N/A Stats/Mean [0.25, 0.276, 0.207] Stats/Standard Deviation [0.151, 0.186, 0.224]"},{"location":"datasets/grape_detection_syntheticday/#examples","title":"Examples","text":""},{"location":"datasets/guava_disease_pakistan/","title":"<code>guava_disease_pakistan</code>","text":""},{"location":"datasets/guava_disease_pakistan/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Canker, Dot, Mummification, Rust Machine Learning Task image_classification Agricultural Task disease_classification Location Pakistan, Asia Sensor Modality RGB Platform ground Input Data Format JPG Annotation Format directory_names Number of Images 306 Documentation https://data.mendeley.com/datasets/s8x6jn5cvr/1 Stats/Mean [0.446, 0.451, 0.323] Stats/Standard Deviation [0.187, 0.184, 0.187]"},{"location":"datasets/guava_disease_pakistan/#examples","title":"Examples","text":""},{"location":"datasets/leaf_counting_denmark/","title":"<code>leaf_counting_denmark</code>","text":""},{"location":"datasets/leaf_counting_denmark/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task image_classification Agricultural Task leaf_counting Location Denmark, Europe Platform mixed Input Data Format PNG Annotation Format directory_names Number of Images 9372 Documentation https://vision.eng.au.dk/leaf-counting-dataset/ Classes 1, 2, 3, 4, 5, 6, 7, 8, 9+ Stats/Mean [0.427, 0.427, 0.342] Stats/Standard Deviation [0.123, 0.143, 0.115]"},{"location":"datasets/leaf_counting_denmark/#examples","title":"Examples","text":""},{"location":"datasets/mango_detection_australia/","title":"<code>mango_detection_australia</code>","text":""},{"location":"datasets/mango_detection_australia/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task object_detection Agricultural Task fruit_detection Location Australia, Oceania Sensor Modality RGB Real or Synthetic real Platform handheld Input Data Format JPG Annotation Format coco_json Number of Images 1730 Documentation https://researchdata.edu.au/mangoyolo-set/1697505 Stats/Mean [0.073, 0.119, 0.05] Stats/Standard Deviation [0.059, 0.079, 0.043] Classes mango"},{"location":"datasets/mango_detection_australia/#examples","title":"Examples","text":""},{"location":"datasets/peachpear_flower_segmentation/","title":"<code>peachpear_flower_segmentation</code>","text":""},{"location":"datasets/peachpear_flower_segmentation/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Flower Machine Learning Task semantic_segmentation Agricultural Task flower_segmentation Location Worldwide Number of Images 42 Documentation https://data.nal.usda.gov/dataset/data-multi-species-fruit-flower-detection-using-refined-semantic-segmentation-network Stats/Mean [0.438, 0.435, 0.415] Stats/Standard Deviation [0.208, 0.214, 0.244]"},{"location":"datasets/peachpear_flower_segmentation/#examples","title":"Examples","text":""},{"location":"datasets/plant_doc_classification/","title":"<code>plant_doc_classification</code>","text":""},{"location":"datasets/plant_doc_classification/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Corn leaf blight, Tomato Early blight leaf, Potato leaf early blight, Potato leaf late blight, Blueberry leaf, grape leaf black rot, Bell_pepper leaf spot, Cherry leaf, Peach leaf, Soyabean leaf, Strawberry leaf, Apple Scab Leaf, Corn rust leaf, Apple leaf, Corn Gray leaf spot, Tomato leaf mosaic virus, Tomato mold leaf, Tomato leaf yellow virus, Tomato leaf bacterial spot, Tomato leaf late blight, Squash Powdery mildew leaf, Bell_pepper leaf, grape leaf, Apple rust leaf, Tomato Septoria leaf spot, Tomato leaf, Raspberry leaf, Tomato two spotted spider mites leaf Machine Learning Task image_classification Agricultural Task disease_classification Location Worldwide Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format JPG Annotation Format directory_names Number of Images 2598 Documentation https://github.com/pratikkayal/PlantDoc-Dataset Stats/Mean [0.482, 0.55, 0.383] Stats/Standard Deviation [0.212, 0.204, 0.221]"},{"location":"datasets/plant_doc_classification/#examples","title":"Examples","text":""},{"location":"datasets/plant_doc_detection/","title":"<code>plant_doc_detection</code>","text":""},{"location":"datasets/plant_doc_detection/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Corn leaf blight, Tomato Early blight leaf, Potato leaf early blight, Potato leaf late blight, Blueberry leaf, grape leaf black rot, Bell_pepper leaf spot, Cherry leaf, Peach leaf, Soyabean leaf, Strawberry leaf, Apple Scab Leaf, Corn rust leaf, Apple leaf, Corn Gray leaf spot, Tomato leaf mosaic virus, Tomato mold leaf, Tomato leaf yellow virus, Tomato leaf bacterial spot, Tomato leaf late blight, Squash Powdery mildew leaf, Bell_pepper leaf, grape leaf, Apple rust leaf, Tomato Septoria leaf spot, Tomato leaf, Raspberry leaf, Potato leaf, Tomato two spotted spider mites leaf Machine Learning Task object_detection Agricultural Task disease_classification Location Worldwide Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format JPG Annotation Format coco_json Number of Images 2346 Documentation https://github.com/pratikkayal/PlantDoc-Object-Detection-Dataset Stats/Mean [0.477, 0.544, 0.378] Stats/Standard Deviation [0.211, 0.204, 0.218]"},{"location":"datasets/plant_doc_detection/#examples","title":"Examples","text":""},{"location":"datasets/plant_seedlings_aarhus/","title":"<code>plant_seedlings_aarhus</code>","text":""},{"location":"datasets/plant_seedlings_aarhus/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task image_classification Agricultural Task weed_classification Location Denmark, Europe Sensor Modality RGB Real or Synthetic real Platform ground_fixed Input Data Format PNG Annotation Format directory_names Number of Images 5539 Documentation https://vision.eng.au.dk/plant-seedlings-dataset/ Classes maize, common_wheat, sugar_beet, scentless_mayweed, common_chickweed, shepherds_purse, cleavers, charlock, fat_hen, smallflowered_cranesbill, black_grass, loose_silkybent Stats/Mean [0.329, 0.289, 0.207] Stats/Standard Deviation [0.093, 0.096, 0.106]"},{"location":"datasets/plant_seedlings_aarhus/#examples","title":"Examples","text":""},{"location":"datasets/plant_village_classification/","title":"<code>plant_village_classification</code>","text":""},{"location":"datasets/plant_village_classification/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task image_classification Agricultural Task disease_classification Location United States, North America Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format JPG Annotation Format directory_names Number of Images 55448 Documentation https://github.com/spMohanty/PlantVillage-Dataset Classes AppleApplescab, AppleBlack_rot, Apple___Cedar_apple_rust, Apple___healthy, Background_without_leaves, Blueberry___healthy, Cherry___Powdery_mildew, Cherry___healthy, Corn___Cercospora_leaf_spot Gray_leaf_spot, Corn___Common_rust, Corn___Northern_Leaf_Blight, Corn___healthy, Grape___Black_rot, Grape___Esca(Black_Measles), GrapeLeafblight_(Isariopsis_Leaf_Spot), Grapehealthy, Orange___Haunglongbing(Citrus_greening), PeachBacterialspot, Peach_healthy, Pepper,_bellBacterialspot, Pepper,_bell_healthy, PotatoEarlyblight, Potato_Late_blight, Potatohealthy, Raspberryhealthy, Soybeanhealthy, SquashPowdery_mildew, StrawberryLeafscorch, Strawberry_healthy, TomatoBacterialspot, Tomato_Early_blight, TomatoLateblight, Tomato_Leaf_Mold, TomatoSeptorialeaf_spot, Tomato_Spider_mites Two-spotted_spider_mite, TomatoTargetSpot, Tomato_Tomato_Yellow_Leaf_Curl_Virus, TomatoTomatomosaic_virus, Tomato_healthy Stats/Mean [0.467, 0.489, 0.412] Stats/Standard Deviation [0.177, 0.152, 0.194]"},{"location":"datasets/plant_village_classification/#examples","title":"Examples","text":""},{"location":"datasets/rangeland_weeds_australia/","title":"<code>rangeland_weeds_australia</code>","text":""},{"location":"datasets/rangeland_weeds_australia/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task image_classification Agricultural Task weed_classification Location Australia, Oceania Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format JPG Annotation Format directory_names Number of Images 17509 Documentation https://github.com/AlexOlsen/DeepWeeds Classes chinee_apple, lantana, negative, parkinsonia, parthenium, prickly_acacia, rubber_vine, siam_weed, snake_weed Stats/Mean [0.379, 0.39, 0.38] Stats/Standard Deviation [0.224, 0.225, 0.223]"},{"location":"datasets/rangeland_weeds_australia/#examples","title":"Examples","text":""},{"location":"datasets/red_grapes_and_leaves_segmentation/","title":"<code>red_grapes_and_leaves_segmentation</code>","text":""},{"location":"datasets/red_grapes_and_leaves_segmentation/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Leaf, Grape Machine Learning Task semantic_segmentation Agricultural Task vineyard_scene_segmentation Location Greece, Europe Sensor Modality RGB Real or Synthetic real Platform handheld/ground Input Data Format JPG Annotation Format image Number of Images 258 Documentation https://link.springer.com/chapter/10.1007/978-3-030-48791-1_22 Stats/Mean [0.392, 0.493, 0.525] Stats/Standard Deviation [0.272, 0.287, 0.283]"},{"location":"datasets/red_grapes_and_leaves_segmentation/#examples","title":"Examples","text":""},{"location":"datasets/rice_seedling_segmentation/","title":"<code>rice_seedling_segmentation</code>","text":""},{"location":"datasets/rice_seedling_segmentation/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task semantic_segmentation Agricultural Task weed_segmentation Location China, Asia Sensor Modality RGB Real or Synthetic real Platform handheld Input Data Format JPG Annotation Format image Number of Images 224 Documentation https://github.com/kabbas570/CED-Net-Crops-and-Weeds-Segmentation-for-Smart-Farming-Using-a-Small-Cascaded-Encoder-Decoder-Archi Stats/Mean [0.655, 0.701, 0.635] Stats/Standard Deviation [0.085, 0.108, 0.111] Classes Rice, Weed"},{"location":"datasets/rice_seedling_segmentation/#examples","title":"Examples","text":""},{"location":"datasets/riseholme_strawberry_classification_2021/","title":"<code>riseholme_strawberry_classification_2021</code>","text":""},{"location":"datasets/riseholme_strawberry_classification_2021/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes anomalous, occluded, ripe, unripe Machine Learning Task image_classification Agricultural Task disease_classification Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format PNG Annotation Format directory_names Number of Images 3520 Documentation https://github.com/ctyeong/Riseholme-2021 Stats/Mean [0.575, 0.464, 0.364] Stats/Standard Deviation [0.198, 0.192, 0.177]"},{"location":"datasets/riseholme_strawberry_classification_2021/#examples","title":"Examples","text":""},{"location":"datasets/soybean_weed_uav_brazil/","title":"<code>soybean_weed_uav_brazil</code>","text":""},{"location":"datasets/soybean_weed_uav_brazil/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task image_classification Agricultural Task weed_classification Location Brazil, South America Sensor Modality RGB Real or Synthetic real Platform uav Input Data Format tif Annotation Format directory_names Number of Images 15336 Documentation https://data.mendeley.com/datasets/3fmjm7ncc6/2 Classes broadleaf, grass, soil, soybean Stats/Mean [0.329, 0.334, 0.187] Stats/Standard Deviation [0.245, 0.249, 0.157]"},{"location":"datasets/soybean_weed_uav_brazil/#examples","title":"Examples","text":""},{"location":"datasets/sugarbeet_weed_segmentation/","title":"<code>sugarbeet_weed_segmentation</code>","text":""},{"location":"datasets/sugarbeet_weed_segmentation/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task semantic_segmentation Agricultural Task weed_segmentation Location None, None Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format PNG Annotation Format image Number of Images 1931 Documentation https://github.com/inkyusa/weedNet Stats/Mean [0.374, 0.374, 0.374] Stats/Standard Deviation [0.17, 0.17, 0.17] Classes sugar_beet, random_weeds"},{"location":"datasets/sugarbeet_weed_segmentation/#examples","title":"Examples","text":""},{"location":"datasets/sugarcane_damage_usa/","title":"<code>sugarcane_damage_usa</code>","text":""},{"location":"datasets/sugarcane_damage_usa/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Machine Learning Task image_classification Agricultural Task damage_classification Location United States, North America Sensor Modality RGB Platform ground_fixed Input Data Format bmp Annotation Format directory_names Number of Images 153 Documentation https://github.com/The77Lab/SugarcaneBilletsDataset Classes cracked, crushed, no_buds, two_buds, single_damaged_buds, no_damage Stats/Mean [0.244, 0.247, 0.262] Stats/Standard Deviation [0.224, 0.217, 0.192]"},{"location":"datasets/sugarcane_damage_usa/#examples","title":"Examples","text":""},{"location":"datasets/tomato_leaf_disease/","title":"<code>tomato_leaf_disease</code>","text":""},{"location":"datasets/tomato_leaf_disease/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Bacterial Spot, Early Blight, Healthy, Late Blight, Leaf Mold, Septoria Leaf Spot, Spider Mites Two-spotted Spider Mite, Target Spot, Tomato Mosaic Virus, Tomato Yellow Leaf Curl Virus Machine Learning Task image_classification Agricultural Task disease_classification Location Worldwide Sensor Modality RGB Real or Synthetic real Platform handheld Input Data Format JPEG Annotation Format directory_names Number of Images 11000 Documentation https://www.kaggle.com/datasets/kaustubhb999/tomatoleaf?resource=download Stats/Mean [0.453, 0.463, 0.419] Stats/Standard Deviation [0.169, 0.148, 0.185]"},{"location":"datasets/tomato_leaf_disease/#examples","title":"Examples","text":""},{"location":"datasets/vegann_multicrop_presence_segmentation/","title":"<code>vegann_multicrop_presence_segmentation</code>","text":""},{"location":"datasets/vegann_multicrop_presence_segmentation/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes plant Machine Learning Task semantic_segmentation Agricultural Task vegetation_segmentation Location Worldwide Sensor Modality RGB Real or Synthetic real Platform aerial Input Data Format PNG Annotation Format image Number of Images 3775 Documentation https://zenodo.org/records/7636408"},{"location":"datasets/vegann_multicrop_presence_segmentation/#examples","title":"Examples","text":""},{"location":"datasets/vine_virus_photo_dataset/","title":"<code>vine_virus_photo_dataset</code>","text":""},{"location":"datasets/vine_virus_photo_dataset/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Leafroll 3, No Virus, Other Red, Red Blotch Machine Learning Task image_classification Agricultural Task vine_virus_photo Location Worldwide Sensor Modality RGB Real or Synthetic real Platform handheld/ground Input Data Format jpg, png, jpeg, JPG Annotation Format directory_names Number of Images 3866 Documentation None Stats/Mean [0.45, 0.449, 0.295] Stats/Standard Deviation [0.231, 0.228, 0.23]"},{"location":"datasets/vine_virus_photo_dataset/#examples","title":"Examples","text":""},{"location":"datasets/wheat_head_counting/","title":"<code>wheat_head_counting</code>","text":""},{"location":"datasets/wheat_head_counting/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Wheat Head Machine Learning Task object_detection Agricultural Task wheat_head_counting Location Worldwide Sensor Modality RGB Real or Synthetic real Platform ground Input Data Format JPG Annotation Format coco_json Number of Images 6512 Documentation https://zenodo.org/record/5092309 Stats/Mean [0.371, 0.361, 0.23] Stats/Standard Deviation [0.219, 0.217, 0.177]"},{"location":"datasets/wheat_head_counting/#examples","title":"Examples","text":""},{"location":"datasets/white_grapes_and_leaves_segmentation/","title":"<code>white_grapes_and_leaves_segmentation</code>","text":""},{"location":"datasets/white_grapes_and_leaves_segmentation/#dataset-metadata","title":"Dataset Metadata","text":"Metadata Value Classes Leaf, Grape Machine Learning Task semantic_segmentation Agricultural Task vineyard_scene_segmentation Location Greece, Europe Sensor Modality RGB Real or Synthetic real Platform handheld/ground Input Data Format JPG Annotation Format image Number of Images 273 Documentation https://link.springer.com/chapter/10.1007/978-3-030-48791-1_22 Stats/Mean [0.309, 0.473, 0.524] Stats/Standard Deviation [0.233, 0.246, 0.253]"},{"location":"datasets/white_grapes_and_leaves_segmentation/#examples","title":"Examples","text":""},{"location":"coverage/","title":"Coverage report","text":""}]}